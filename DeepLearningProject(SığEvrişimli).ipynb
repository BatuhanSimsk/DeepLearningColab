{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d tawsifurrahman/covid19-radiography-database"
      ],
      "metadata": {
        "id": "--bvXoYRyY7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f08fc0a7-d5eb-438f-b998-b44f07e7a373"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.6.14 / client 1.6.12)\n",
            "Dataset URL: https://www.kaggle.com/datasets/tawsifurrahman/covid19-radiography-database\n",
            "License(s): copyright-authors\n",
            "Downloading covid19-radiography-database.zip to /content\n",
            "100% 777M/778M [00:36<00:00, 24.0MB/s]\n",
            "100% 778M/778M [00:36<00:00, 22.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gwbD20uYyHPE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ZIP dosyasının yolu\n",
        "zip_file_path = '/content/covid19-radiography-database.zip'\n",
        "\n",
        "# Çıkartılacak hedef klasör\n",
        "extracted_folder_path = '/content/'\n",
        "\n",
        "# ZIP dosyasını aç\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "f0Dkvp6gyojR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data_dir = '/content/COVID-19_Radiography_Dataset'\n",
        "\n",
        "train_dir = '/content/TrainTemp'\n",
        "test_dir = '/content/Test'\n",
        "\n",
        "classes = ['COVID', 'Normal', 'Viral Pneumonia']\n",
        "\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Her sınıf için eğitim ve test dizinlerini oluştur\n",
        "for cls in classes:\n",
        "    os.makedirs(os.path.join(train_dir, cls), exist_ok=True)\n",
        "    os.makedirs(os.path.join(test_dir, cls), exist_ok=True)\n",
        "\n",
        "# Her sınıf için görsel dosyalarını eğitim ve test dizinlerine kopyala\n",
        "for cls in classes:\n",
        "    cls_dir = os.path.join(data_dir, cls, 'images')\n",
        "    images = os.listdir(cls_dir)\n",
        "    train_images, test_images = train_test_split(images, test_size=0.2, random_state=99)\n",
        "\n",
        "    for img in train_images:\n",
        "        src = os.path.join(cls_dir, img)\n",
        "        dst = os.path.join(train_dir, cls, img)\n",
        "        shutil.copy(src, dst)\n",
        "\n",
        "    for img in test_images:\n",
        "        src = os.path.join(cls_dir, img)\n",
        "        dst = os.path.join(test_dir, cls, img)\n",
        "        shutil.copy(src, dst)\n"
      ],
      "metadata": {
        "id": "NKCC0nmDpDwk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_dir = '/content/TrainTemp'\n",
        "\n",
        "train_data_dir = '/content/Train'\n",
        "validation_data_dir = '/content/Validation'\n",
        "test_data_dir = '/content/Test'  # Test veri setinin bulunduğu dizin\n",
        "\n",
        "os.makedirs(train_data_dir, exist_ok=True)\n",
        "os.makedirs(validation_data_dir, exist_ok=True)\n",
        "\n",
        "classes = ['COVID', 'Normal', 'Viral Pneumonia']\n",
        "\n",
        "# Her sınıf için eğitim ve doğrulama dizinlerini oluştur\n",
        "for cls in classes:\n",
        "    os.makedirs(os.path.join(train_data_dir, cls), exist_ok=True)\n",
        "    os.makedirs(os.path.join(validation_data_dir, cls), exist_ok=True)\n",
        "\n",
        "# Her sınıf için eğitim verisini eğitim ve doğrulama setlerine ayır\n",
        "for cls in classes:\n",
        "    cls_dir = os.path.join(train_dir, cls)\n",
        "    train_images = os.listdir(cls_dir)\n",
        "    train_images, validation_images = train_test_split(train_images, test_size=0.125, random_state=99)\n",
        "\n",
        "    for img in train_images:\n",
        "        src = os.path.join(cls_dir, img)\n",
        "        dst = os.path.join(train_data_dir, cls, img)\n",
        "        shutil.copy(src, dst)\n",
        "\n",
        "    for img in validation_images:\n",
        "        src = os.path.join(cls_dir, img)\n",
        "        dst = os.path.join(validation_data_dir, cls, img)\n",
        "        shutil.copy(src, dst)\n",
        "\n",
        "\n",
        "import shutil\n",
        "\n",
        "shutil.rmtree(\"/content/TrainTemp\")"
      ],
      "metadata": {
        "id": "fXwTTm-5pLhv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Veri setinin dizini\n",
        "data_dir = \"/content\"\n",
        "\n",
        "# Veri ön işleme ve artırma\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Veri yükleyicileri oluşturma\n",
        "train_dataset = datasets.ImageFolder(root=data_dir + '/Train', transform=data_transforms)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "\n",
        "test_dataset = datasets.ImageFolder(root=data_dir + '/Test', transform=data_transforms)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n"
      ],
      "metadata": {
        "id": "EvXBjdcTzXxj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec21c176-ace9-4c16-a515-e3da9c3c9077"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ShallowCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ShallowCNN, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.fc1 = nn.Linear(32 * 56 * 56, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = x.view(x.size(0), -1)  # Düzleştirme\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "2tMJgrPbza2J"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model parametreleri\n",
        "num_classes = len(train_dataset.classes)\n",
        "\n",
        "# Modeli oluştur\n",
        "model = ShallowCNN(num_classes)\n",
        "\n",
        "# Eğitim için CUDA kullanılabilirse GPU'ya taşıma\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n"
      ],
      "metadata": {
        "id": "pukvymEBzeC2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\", colour='green', leave=True)\n",
        "    for batch_idx, (inputs, labels) in progress_bar:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Gradientleri sıfırlama\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass ve optimizasyon\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        progress_bar.set_postfix({'Loss': loss.item()})\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ZdJJGEPDzg06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f83a1d1-94b4-4f2a-84de-b1e8bf1557d7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Epoch 1/3: 100%|\u001b[32m██████████\u001b[0m| 332/332 [00:44<00:00,  7.49batch/s, Loss=0.619]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Loss: 0.6261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 2/3: 100%|\u001b[32m██████████\u001b[0m| 332/332 [00:43<00:00,  7.64batch/s, Loss=0.106]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/3, Loss: 0.2786\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 3/3: 100%|\u001b[32m██████████\u001b[0m| 332/332 [00:47<00:00,  7.04batch/s, Loss=0.0946]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Loss: 0.2157\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "TP = 0\n",
        "FP = 0\n",
        "FN = 0\n",
        "TN = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Kesinlik ve Duyarlılık için TP, FP, FN ve TN değerlerini hesapla\n",
        "        TP += ((predicted == labels) & (labels == 1)).sum().item()\n",
        "        FP += ((predicted != labels) & (labels == 0)).sum().item()\n",
        "        FN += ((predicted != labels) & (labels == 1)).sum().item()\n",
        "        TN += ((predicted == labels) & (labels == 0)).sum().item()\n",
        "\n",
        "# Doğruluk (Accuracy) hesapla\n",
        "accuracy = correct / total\n",
        "\n",
        "# Kesinlik (Precision) hesapla\n",
        "precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
        "\n",
        "# Duyarlılık (Recall) hesapla\n",
        "recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
        "\n",
        "# F1-Skoru hesapla\n",
        "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1_score:.4f}\")\n",
        "\n",
        "print(f\"TP: {TP}\")\n",
        "print(f\"FP: {FP}\")\n",
        "print(f\"FN: {FN}\")\n",
        "print(f\"TN: {TN}\")"
      ],
      "metadata": {
        "id": "lnP0QIULzor5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58a582fb-a298-433b-f7f1-ac637607a516"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.8829\n",
            "Precision: 0.8993\n",
            "Recall: 0.9676\n",
            "F1-Score: 0.9322\n",
            "TP: 1973\n",
            "FP: 221\n",
            "FN: 66\n",
            "TN: 503\n"
          ]
        }
      ]
    }
  ]
}